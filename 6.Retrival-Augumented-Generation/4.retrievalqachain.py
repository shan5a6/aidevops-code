# =========================================================
# LANGCHAIN - COMPLETE RAG PIPELINE WITH GROQ
# =========================================================

# 1Ô∏è‚É£ Load environment variables (good practice for future API keys)
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())

# ---------------------------------------------------------
# 2Ô∏è‚É£ Load the document
from langchain_community.document_loaders import TextLoader

# Make sure you have a file named 'bella_vista.txt' in this path
loader = TextLoader("/root/aidevops/aidevops-langchain/6.Retrival-Augumented-Generation/bella_vista.txt")
docs = loader.load()

print(f"‚úÖ Loaded {len(docs)} document(s)")
print("Sample Document:", docs[0])

# ---------------------------------------------------------
# 3Ô∏è‚É£ Split document into smaller chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,  # number of characters per chunk
    chunk_overlap=20 # overlap to avoid losing context
)

documents = text_splitter.split_documents(docs)
print(f"‚úÖ Created {len(documents)} chunks")
print("Sample Chunk:", documents[0])

# ---------------------------------------------------------
# 4Ô∏è‚É£ Create embeddings (Convert text ‚Üí vectors)
from langchain_huggingface import HuggingFaceEmbeddings

# Using open-source, lightweight embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Test embedding for a query
query_vector = embeddings.embed_query("What are the opening hours?")
print(f"‚úÖ Query vector length: {len(query_vector)}")

# ---------------------------------------------------------
# 5Ô∏è‚É£ Store chunks in FAISS vector database
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(documents, embeddings)
vectorstore.save_local("bella_vista_index")
print("‚úÖ Vectorstore created and saved locally")

# ---------------------------------------------------------
# 6Ô∏è‚É£ Load the saved vectorstore and create retriever
vectorstore = FAISS.load_local("bella_vista_index", embeddings, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever()

# ---------------------------------------------------------
# 7Ô∏è‚É£ Initialize Groq LLM and Prompt Template
from langchain_groq import ChatGroq
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain.prompts import PromptTemplate

# Define the prompt template
prompt_template = """You are a helpful assistant for our restaurant.

{context}

Question: {input}
Answer here:"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "input"]
)

# Initialize Groq LLM
llm = ChatGroq(model="llama-3.3-70b-versatile")

# Create documents chain
combine_docs_chain = create_stuff_documents_chain(llm, PROMPT)

# Create retrieval chain
qa = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=combine_docs_chain
)

# ---------------------------------------------------------
# 8Ô∏è‚É£ Example Queries
print("\nüîé Example Query: When are the opening hours?")
result1 = qa.invoke({"input": "When are the opening hours on Sunday?"})
# print(result1)
print(result1.get("answer"))

print("\nüîé Example Query: Do you offer vegetarian or vegan options?")
result2 = qa.invoke({"input": "Do you offer vegetarian or vegan options?"})
# print(result2)
print(result2.get("answer"))

print("\nüîé Example Query: Can I book a private event?")
result3 = qa.invoke({"input": "Can I book a private event?"})
# print(result3)
print(result3.get("answer"))

# =========================================================
# END OF SCRIPT
# =========================================================
